{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35edeb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (4.56.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: evaluate in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (0.4.5)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from transformers) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from transformers) (0.22.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading scikit_learn-1.7.2-cp310-cp310-win_amd64.whl (8.9 MB)\n",
      "   ---------------------------------------- 0.0/8.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/8.9 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.1/8.9 MB 9.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.2/8.9 MB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.6/8.9 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.9/8.9 MB 9.5 MB/s  0:00:00\n",
      "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ---------------------------------------- 4/4 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.2 scikit-learn-1.7.2 scipy-1.15.3 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers datasets torch accelerate evaluate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fe9eec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [168/168 15:02, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.647554</td>\n",
       "      <td>0.580000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.441512</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.482431</td>\n",
       "      <td>0.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.458393</td>\n",
       "      <td>0.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.503350</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.540378</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.643764</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.700268</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評估結果: {'eval_loss': 0.44151151180267334, 'eval_accuracy': 0.8, 'eval_runtime': 5.0401, 'eval_samples_per_second': 19.841, 'eval_steps_per_second': 0.397, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_bert\\\\tokenizer_config.json',\n",
       " './fine_tuned_bert\\\\special_tokens_map.json',\n",
       " './fine_tuned_bert\\\\vocab.txt',\n",
       " './fine_tuned_bert\\\\added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 匯入必要的庫\n",
    "from datasets import load_dataset  # 用來載入資料集\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments  # BERT 相關工具\n",
    "import torch  # PyTorch 框架\n",
    "import evaluate  # 用來評估模型效能\n",
    "\n",
    "# Step 1: 載入 IMDb 資料集（情感分析：正面/負面評論）\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(500))\n",
    "eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(100))\n",
    "\n",
    "# Step 2: 載入 BERT Tokenizer（用來將文字轉換成模型輸入）\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 定義 Tokenization 函數\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)  # 限制長度為 128 tokens\n",
    "\n",
    "# 應用 Tokenization 到資料集\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Step 3: 載入 BERT 模型（用於序列分類，num_labels=2 表示二元分類）\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "\n",
    "################################-TO DO-#################################################\n",
    "# Step 4: 設定訓練參數\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",  # 輸出目錄\n",
    "    num_train_epochs=8,  # 訓練 epoch 數\n",
    "    per_device_train_batch_size=24,  # 每個裝置的 batch size\n",
    "    per_device_eval_batch_size=64,  # 評估時的 batch size\n",
    "    warmup_steps=24,  # 學習率 warmup\n",
    "    weight_decay=0.005,  # 權重衰減\n",
    "    logging_dir=\"./logs\",  # 日誌目錄\n",
    "    eval_strategy=\"epoch\",  # 每個 epoch 評估一次\n",
    "    save_strategy=\"epoch\",  # 每個 epoch 保存模型\n",
    "    load_best_model_at_end=True,  # 訓練結束載入最佳模型\n",
    "    report_to=\"none\",\n",
    ")\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "# Step 5: 定義評估指標（使用 accuracy）\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Step 6: 使用 Trainer API 進行訓練\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 開始訓練\n",
    "trainer.train()\n",
    "\n",
    "# Step 7: 評估模型\n",
    "results = trainer.evaluate()\n",
    "print(\"評估結果:\", results)\n",
    "\n",
    "# Step 8: 儲存模型（可選）\n",
    "trainer.save_model(\"./fine_tuned_bert\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8afdf3fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    label  confidence\n",
      "0  This movie is sooooo nice!! I should watch it ...  LABEL_1    0.892257\n",
      "1  Absolutely loved the soundtrack, it fit perfec...  LABEL_1    0.921078\n",
      "2         One of the best films I’ve seen this year!  LABEL_1    0.913377\n",
      "3   Amazing visuals, the cinematography is stunning.  LABEL_1    0.930297\n",
      "4         A masterpiece, truly unforgettable cinema.  LABEL_1    0.832912\n",
      "5   I wouldn’t recommend it, felt too long and slow.  LABEL_0    0.837048\n",
      "6        Characters were shallow and underdeveloped.  LABEL_0    0.805675\n",
      "7         Not worth the hype, kind of disappointing.  LABEL_0    0.854372\n",
      "8     The story didn’t make sense, left me confused.  LABEL_0    0.842527\n",
      "9  It was painful to sit through, very disappoint...  LABEL_0    0.839201\n"
     ]
    }
   ],
   "source": [
    "# Step 9: 使用 fine-tuned 模型預測 CSV 資料\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# 載入 fine-tuned 模型\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"./fine_tuned_bert\")\n",
    "\n",
    "# 讀取 CSV 檔案\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "predictions = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['data']\n",
    "    result = classifier(text)\n",
    "    predictions.append({\n",
    "        'text': text,\n",
    "        'label': result[0]['label'],\n",
    "        'confidence': result[0]['score']\n",
    "    })\n",
    "    \n",
    "# 將預測結果轉換為 DataFrame\n",
    "results_df = pd.DataFrame(predictions)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59324401",
   "metadata": {},
   "source": [
    "### 更多測試~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc510a",
   "metadata": {},
   "source": [
    "用自己的資料好像不太行qq (but是財經新聞，跟訓練的電影評論資料有點差距:3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b504b4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.6631180047988892}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 載入 fine-tuned 模型\n",
    "classifier = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"./fine_tuned_bert\",\n",
    "    device=0  # 強制用 GPU\n",
    ")\n",
    "\n",
    "# 要分析的句子(應該是正面LABEL_1。)\n",
    "text = \"高盛證券於舊金山舉行論壇，邀請台積電高層進行深度對談，聚焦四大主題：智慧機與高效能運算（HPC）帶動先進製程需求。\"\n",
    "\n",
    "# 預測\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fec03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9028972387313843}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 載入 fine-tuned 模型\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"./fine_tuned_bert\")\n",
    "\n",
    "# 要分析的句子(應該是正面LABEL_1。)\n",
    "text = \"Goldman Sachs held a forum in San Francisco, inviting TSMC executives for an in-depth discussion, focusing on four key themes: Smartphones and high-performance computing (HPC) drive demand for advanced processes.\"\n",
    "\n",
    "# 預測\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8610269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.8271191716194153}]\n"
     ]
    }
   ],
   "source": [
    "# 要分析的句子(應該是負面LABEL_0。)\n",
    "text = \"America's jobs market is in a 'two-tier growth spurt.\"\n",
    "\n",
    "# 預測\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7ab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.8971593976020813}]\n"
     ]
    }
   ],
   "source": [
    "# 要分析的句子(應該是負面LABEL_0。)\n",
    "text = \"\"\"It’s signaling that we’re in a two-tier growth spurt,\n",
    " Richardson said on the CNN Business digital live show Markets Now.\n",
    "The first tier is that the nation is still filling back the jobs lost during the worst of the pandemic. \n",
    "The second is that the economy is actually changing, Richardson said.\"\"\"\n",
    "\n",
    "# 預測\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0df9d5",
   "metadata": {},
   "source": [
    "### 試試看增強版的(更多訓練資料，更長的token。)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed52ce",
   "metadata": {},
   "source": [
    "分析英文電影評論貌似步錯，但其餘表現似乎持續尷尬，用電影評論預測財經新聞，或是分析中文評論好似真不太適合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "201c608a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\bert_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8000/8000 [00:42<00:00, 187.57 examples/s]\n",
      "Map: 100%|██████████| 1500/1500 [00:07<00:00, 200.34 examples/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1336' max='1336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1336/1336 26:38, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.396433</td>\n",
       "      <td>0.832667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.285562</td>\n",
       "      <td>0.875333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.400300</td>\n",
       "      <td>0.265365</td>\n",
       "      <td>0.899333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.400300</td>\n",
       "      <td>0.363845</td>\n",
       "      <td>0.867333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.400300</td>\n",
       "      <td>0.426155</td>\n",
       "      <td>0.878000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.459131</td>\n",
       "      <td>0.888667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.464007</td>\n",
       "      <td>0.890667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.118400</td>\n",
       "      <td>0.625987</td>\n",
       "      <td>0.855333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='12' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12/12 00:38]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "評估結果: {'eval_loss': 0.26536479592323303, 'eval_accuracy': 0.8993333333333333, 'eval_runtime': 42.269, 'eval_samples_per_second': 35.487, 'eval_steps_per_second': 0.284, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_bert_max\\\\tokenizer_config.json',\n",
       " './fine_tuned_bert_max\\\\special_tokens_map.json',\n",
       " './fine_tuned_bert_max\\\\vocab.txt',\n",
       " './fine_tuned_bert_max\\\\added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 匯入必要的庫\n",
    "from datasets import load_dataset  # 用來載入資料集\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments  # BERT 相關工具\n",
    "import torch  # PyTorch 框架\n",
    "import evaluate  # 用來評估模型效能\n",
    "\n",
    "# 檢查 CUDA 是否可用\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Step 1: 載入 IMDb 資料集（情感分析：正面/負面評論）\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select(range(8000))\n",
    "eval_dataset = dataset[\"test\"].shuffle(seed=42).select(range(1500))\n",
    "\n",
    "# Step 2: 載入 BERT Tokenizer（用來將文字轉換成模型輸入）\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 定義 Tokenization 函數\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=200)  # 限制長度為 200 tokens\n",
    "\n",
    "# 應用 Tokenization 到資料集\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Step 3: 載入 BERT 模型（用於序列分類，num_labels=2 表示二元分類）\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "\n",
    "################################-TO DO-#################################################\n",
    "# Step 4: 設定訓練參數\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Upg.results\",\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=48,  # 依 GPU 記憶體調整\n",
    "    per_device_eval_batch_size=128,  # 評估可設大\n",
    "    warmup_steps=1500,                # 約 5~10% 總步驟\n",
    "    weight_decay=0.005,               # 可微調，0.005~0.01\n",
    "    logging_dir=\"./logs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "# Step 5: 定義評估指標（使用 accuracy）\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = torch.argmax(torch.tensor(logits), dim=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Step 6: 使用 Trainer API 進行訓練\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 開始訓練\n",
    "trainer.train()\n",
    "\n",
    "# Step 7: 評估模型\n",
    "results = trainer.evaluate()\n",
    "print(\"評估結果:\", results)\n",
    "\n",
    "# Step 8: 儲存模型\n",
    "trainer.save_model(\"./fine_tuned_bert_max\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_bert_max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b17fc19",
   "metadata": {},
   "source": [
    "#### 這邊更強的模型，表現的confidence更好了~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43be7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text    label  confidence\n",
      "0  This movie is sooooo nice!! I should watch it ...  LABEL_1    0.917468\n",
      "1  Absolutely loved the soundtrack, it fit perfec...  LABEL_1    0.984024\n",
      "2         One of the best films I’ve seen this year!  LABEL_1    0.972332\n",
      "3   Amazing visuals, the cinematography is stunning.  LABEL_1    0.985718\n",
      "4         A masterpiece, truly unforgettable cinema.  LABEL_1    0.956977\n",
      "5   I wouldn’t recommend it, felt too long and slow.  LABEL_0    0.895157\n",
      "6        Characters were shallow and underdeveloped.  LABEL_0    0.936893\n",
      "7         Not worth the hype, kind of disappointing.  LABEL_0    0.934566\n",
      "8     The story didn’t make sense, left me confused.  LABEL_0    0.916409\n",
      "9  It was painful to sit through, very disappoint...  LABEL_0    0.942754\n"
     ]
    }
   ],
   "source": [
    "# Step 9: 使用更強的 fine-tuned 模型預測 CSV 資料\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "# 載入 fine-tuned 模型\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"./fine_tuned_bert_max\")\n",
    "\n",
    "# 讀取 CSV 檔案\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "predictions = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    text = row['data']\n",
    "    result = classifier(text)\n",
    "    predictions.append({\n",
    "        'text': text,\n",
    "        'label': result[0]['label'],\n",
    "        'confidence': result[0]['score']\n",
    "    })\n",
    "    \n",
    "# 將預測結果轉換為 DataFrame\n",
    "results_df = pd.DataFrame(predictions)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85139022",
   "metadata": {},
   "source": [
    "#### 電影評論進階題\n",
    "進一步比較中英文的差距，英文表現幾乎正確，中文錯誤偏多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69a0d38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 這部電影雖然節奏緩慢，但最後的情感爆發讓人感動。\n",
      "預測: LABEL_0 (信心: 0.55)，正確答案: POSITIVE\n",
      "\n",
      "2. 演員表現不錯，可惜劇情讓人失望。\n",
      "預測: LABEL_0 (信心: 0.66)，正確答案: NEGATIVE\n",
      "\n",
      "3. 一開始覺得無聊，沒想到後面越來越精彩。\n",
      "預測: LABEL_0 (信心: 0.58)，正確答案: POSITIVE\n",
      "\n",
      "4. 特效很棒，但故事完全沒吸引力。\n",
      "預測: LABEL_0 (信心: 0.60)，正確答案: NEGATIVE\n",
      "\n",
      "5. 雖然有些地方拖戲，但整體來說還算不錯。\n",
      "預測: LABEL_0 (信心: 0.65)，正確答案: POSITIVE\n",
      "\n",
      "6. 配樂很出色，可惜角色塑造太薄弱。\n",
      "預測: LABEL_0 (信心: 0.64)，正確答案: NEGATIVE\n",
      "\n",
      "7. 不是我喜歡的類型，但結局讓我改觀。\n",
      "預測: LABEL_0 (信心: 0.63)，正確答案: POSITIVE\n",
      "\n",
      "8. 畫面很美，可惜劇情漏洞太多。\n",
      "預測: LABEL_0 (信心: 0.61)，正確答案: NEGATIVE\n",
      "\n",
      "9. 雖然有些老套，但還是讓人看得很開心。\n",
      "預測: LABEL_0 (信心: 0.63)，正確答案: POSITIVE\n",
      "\n",
      "10. 導演很有想法，但整體表現不如預期。\n",
      "預測: LABEL_0 (信心: 0.64)，正確答案: NEGATIVE\n",
      "\n",
      "1. Although the movie is slow-paced, the emotional climax at the end is touching.\n",
      "Prediction: LABEL_1 (Confidence: 0.99), Correct: POSITIVE\n",
      "\n",
      "2. The actors did well, but the plot was disappointing.\n",
      "Prediction: LABEL_0 (Confidence: 0.96), Correct: NEGATIVE\n",
      "\n",
      "3. It felt boring at first, but it got more exciting later on.\n",
      "Prediction: LABEL_1 (Confidence: 0.70), Correct: POSITIVE\n",
      "\n",
      "4. The special effects were great, but the story was not engaging at all.\n",
      "Prediction: LABEL_0 (Confidence: 0.97), Correct: NEGATIVE\n",
      "\n",
      "5. Although some parts dragged, overall it was pretty good.\n",
      "Prediction: LABEL_1 (Confidence: 0.74), Correct: POSITIVE\n",
      "\n",
      "6. The soundtrack was excellent, but the character development was too weak.\n",
      "Prediction: LABEL_0 (Confidence: 0.93), Correct: NEGATIVE\n",
      "\n",
      "7. Not my favorite genre, but the ending changed my mind.\n",
      "Prediction: LABEL_0 (Confidence: 0.80), Correct: POSITIVE\n",
      "\n",
      "8. The visuals were beautiful, but there were too many plot holes.\n",
      "Prediction: LABEL_0 (Confidence: 0.95), Correct: NEGATIVE\n",
      "\n",
      "9. Although a bit clichéd, it was still enjoyable to watch.\n",
      "Prediction: LABEL_1 (Confidence: 0.92), Correct: POSITIVE\n",
      "\n",
      "10. The director had ideas, but the overall execution was below expectations.\n",
      "Prediction: LABEL_0 (Confidence: 0.95), Correct: NEGATIVE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10 個很難辨識的電影評論（中英文），並附上正確答案\n",
    "reviews = [\n",
    "    # 1. 難辨識的正面\n",
    "    {\"zh\": \"這部電影雖然節奏緩慢，但最後的情感爆發讓人感動。\", \"en\": \"Although the movie is slow-paced, the emotional climax at the end is touching.\", \"label\": \"POSITIVE\"},\n",
    "    # 2. 難辨識的負面\n",
    "    {\"zh\": \"演員表現不錯，可惜劇情讓人失望。\", \"en\": \"The actors did well, but the plot was disappointing.\", \"label\": \"NEGATIVE\"},\n",
    "    # 3. 難辨識的正面\n",
    "    {\"zh\": \"一開始覺得無聊，沒想到後面越來越精彩。\", \"en\": \"It felt boring at first, but it got more exciting later on.\", \"label\": \"POSITIVE\"},\n",
    "    # 4. 難辨識的負面\n",
    "    {\"zh\": \"特效很棒，但故事完全沒吸引力。\", \"en\": \"The special effects were great, but the story was not engaging at all.\", \"label\": \"NEGATIVE\"},\n",
    "    # 5. 難辨識的正面\n",
    "    {\"zh\": \"雖然有些地方拖戲，但整體來說還算不錯。\", \"en\": \"Although some parts dragged, overall it was pretty good.\", \"label\": \"POSITIVE\"},\n",
    "    # 6. 難辨識的負面\n",
    "    {\"zh\": \"配樂很出色，可惜角色塑造太薄弱。\", \"en\": \"The soundtrack was excellent, but the character development was too weak.\", \"label\": \"NEGATIVE\"},\n",
    "    # 7. 難辨識的正面\n",
    "    {\"zh\": \"不是我喜歡的類型，但結局讓我改觀。\", \"en\": \"Not my favorite genre, but the ending changed my mind.\", \"label\": \"POSITIVE\"},\n",
    "    # 8. 難辨識的負面\n",
    "    {\"zh\": \"畫面很美，可惜劇情漏洞太多。\", \"en\": \"The visuals were beautiful, but there were too many plot holes.\", \"label\": \"NEGATIVE\"},\n",
    "    # 9. 難辨識的正面\n",
    "    {\"zh\": \"雖然有些老套，但還是讓人看得很開心。\", \"en\": \"Although a bit clichéd, it was still enjoyable to watch.\", \"label\": \"POSITIVE\"},\n",
    "    # 10. 難辨識的負面\n",
    "    {\"zh\": \"導演很有想法，但整體表現不如預期。\", \"en\": \"The director had ideas, but the overall execution was below expectations.\", \"label\": \"NEGATIVE\"}\n",
    "]\n",
    "\n",
    "\n",
    "zh_texts = [r[\"zh\"] for r in reviews]\n",
    "zh_labels = [r[\"label\"] for r in reviews]\n",
    "zh_results = classifier(zh_texts)\n",
    "for i, (text, result, label) in enumerate(zip(zh_texts, zh_results, zh_labels), 1):\n",
    "    print(f\"{i}. {text}\\n預測: {result['label']} (信心: {result['score']:.2f})，正確答案: {label}\\n\")\n",
    "\n",
    "\n",
    "en_texts = [r[\"en\"] for r in reviews]\n",
    "en_labels = [r[\"label\"] for r in reviews]\n",
    "en_results = classifier(en_texts)\n",
    "for i, (text, result, label) in enumerate(zip(en_texts, en_results, en_labels), 1):\n",
    "    print(f\"{i}. {text}\\nPrediction: {result['label']} (Confidence: {result['score']:.2f}), Correct: {label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98606022",
   "metadata": {},
   "source": [
    "#### 底下都是財經新聞的情緒分析...目前不太好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "498ed7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9328424334526062}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# 載入 fine-tuned 模型\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"./fine_tuned_bert_max\")\n",
    "\n",
    "# 要分析的句子(應該是正面LABEL_1。)\n",
    "text = \"\"\"Goldman Sachs held a forum in San Francisco, inviting TSMC executives for an in-depth discussion,\n",
    " focusing on four key themes: Smartphones and high-performance computing (HPC) \n",
    " drive demand for advanced processes.\"\"\"\n",
    "\n",
    "# 預測\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498ed7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.6301445364952087}]\n"
     ]
    }
   ],
   "source": [
    "# 要分析的句子(應該是正面LABEL_1。)\n",
    "text = \"高盛證券於舊金山舉行論壇，邀請台積電高層進行深度對談，聚焦四大主題：智慧機與高效能運算（HPC）帶動先進製程需求。\"\n",
    "# 預測\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9f1872e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.6281268000602722}]\n"
     ]
    }
   ],
   "source": [
    "# 要分析的句子(應該是正面LABEL_1。)\n",
    "text = \"\"\"台積電米玉傑等5人成為工研院新院士，這屆新任院士展現台灣在先進製程、工程永續與醫療健康上的厚實能量，\n",
    "不僅強化科技實力，更提升國際影響力。\"\"\"\n",
    "# 預測\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a27e904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.5948212742805481}]\n"
     ]
    }
   ],
   "source": [
    "# 要分析的句子(應該是正面LABEL_1。)\n",
    "text = \"\"\"沒必過度擔心明年台股 前外資天后投資教戰。\"\"\"\n",
    "# 預測\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b2fa8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.7858993411064148}]\n"
     ]
    }
   ],
   "source": [
    "# 要分析的句子(應該是正面LABEL_1。)\n",
    "text = \"\"\"There's no need to worry too much about the Taiwan stock market next year. \n",
    "A former foreign investment queen offers investment advice.\"\"\"\n",
    "# 預測\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "009edc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.7364367246627808}]\n"
     ]
    }
   ],
   "source": [
    "# 要分析的句子(應該是負面LABEL_0。)\n",
    "text = \"America's jobs market is in a 'two-tier growth spurt.\"\n",
    "# 預測\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb46cfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9200599789619446}]\n"
     ]
    }
   ],
   "source": [
    "# 要分析的句子(應該是負面LABEL_0。)\n",
    "text = \"\"\"It’s signaling that we’re in a two-tier growth spurt, Richardson said on the CNN Business digital\n",
    "live show Markets Now. The first tier is that the nation is still filling back the jobs lost during the worst\n",
    "of the pandemic. The second is that the economy is actually changing, Richardson said.\"\"\"\n",
    "# 預測\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5007274c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.7618705630302429}]\n"
     ]
    }
   ],
   "source": [
    "# 要分析的句子(應該是負面LABEL_0。)\n",
    "text = \"\"\"It’s lunchtime and the market looks red.After a mixed morning, where the Dow eked out some time\n",
    " in positive territory, all three major stock indexes are now trading lower.\"\"\"\n",
    "# 預測\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5007274c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.835030198097229}]\n"
     ]
    }
   ],
   "source": [
    "# 要分析的句子(應該是負面LABEL_0。)\n",
    "text = \"\"\"“S&P 500 [earnings per share] is up 50% from the Covid trough, \n",
    "and the S&P 500 has doubled,” the bank’s analysts wrote in a note this morning. \n",
    "But what good news is left?Not very much… the economy is recovering,\n",
    " though the pace of improvements is taking a hit amid the spread of the Delta variant.\n",
    "   Meanwhile, the Federal Reserve is preparing to normalize its ultra-easy money policies.\"\"\"\n",
    "\n",
    "# 預測\n",
    "result = classifier(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7853dd01",
   "metadata": {},
   "source": [
    "新聞標題表現不太優..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d08f6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL_0 (信心: 0.62)\n",
      "LABEL_0 (信心: 0.53)\n",
      "LABEL_0 (信心: 0.67)\n",
      "LABEL_0 (信心: 0.61)\n",
      "LABEL_0 (信心: 0.66)\n",
      "LABEL_0 (信心: 0.58)\n",
      "LABEL_1 (信心: 0.51)\n",
      "LABEL_0 (信心: 0.57)\n",
      "LABEL_1 (信心: 0.59)\n",
      "LABEL_1 (信心: 0.55)\n",
      "LABEL_0 (信心: 0.63)\n",
      "LABEL_0 (信心: 0.58)\n",
      "LABEL_0 (信心: 0.53)\n",
      "LABEL_0 (信心: 0.66)\n",
      "LABEL_1 (信心: 0.54)\n",
      "LABEL_0 (信心: 0.56)\n",
      "LABEL_0 (信心: 0.54)\n",
      "LABEL_0 (信心: 0.53)\n",
      "LABEL_0 (信心: 0.53)\n",
      "LABEL_0 (信心: 0.51)\n"
     ]
    }
   ],
   "source": [
    "# 新增一個 cell，將20個新聞標題丟入 fine-tuned BERT 模型，批次判斷正面/負面\n",
    "from transformers import pipeline\n",
    "# 載入 fine-tuned 模型\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"./fine_tuned_bert_max\")\n",
    "# 20則新聞標題\n",
    "titles = [\n",
    "    \"廣運營收續報喜 好轉已成事實\",\n",
    "    \"地表最強投顧來襲！ 川普喊話「大幅降息」\",\n",
    "    \"智慧醫療熱度不減 安勤鎖定關鍵成長\",\n",
    "    \"台灣無人機聯盟啟航！ 哪些個股最值得關注？\",\n",
    "    \"力成擺脫大牛股印象 成為法人愛股\",\n",
    "    \"英偉達遭中國反壟斷調查 市場該如何看待？\",\n",
    "    \"台灣10家公司入列！《時代》揭全球最佳企業榜單：輝達、微軟⋯以AI為貴，蘋果、台積電卻落榜？\",\n",
    "    \"iPhone 17台灣預購已開！電信三雄資費方案攻略\",\n",
    "    \"出席「大專生洄游農村競賽」...蕭美琴談五大青年照顧政策，勉勵學子將台灣農村精神帶向國際\",\n",
    "    \"台新證1.1264股換元富證1股合併！市佔躍升第4大、期貨合併成第5大…賴昭吟：資源整合發揮綜效\",\n",
    "    \"AI液冷大爆發！雙鴻卡位液冷先機\",\n",
    "    \"8月EPS年增近6成！ 光寶科卡位電源核心\",\n",
    "    \"興達電廠火災將大限電？台電闢謠，盤點近年「大停電」意外一次看\",\n",
    "    \"第三代晶圓連日狂飆 SiC成關鍵解方\",\n",
    "    \"想要一整天大腦清醒？起床第一件事：絕對不要看手機！\",\n",
    "    \"當主管好難！改變快被說假、慢被說不積極？怎麼做才對\",\n",
    "    \"赴韓參訪非軍事區、38度線！謝金河回頭看韓戰：到底南韓還是台灣安全？用1差別看地表最危險國家\",\n",
    "    \"彎著腰前進南韓「非軍事區」坑道！內部畫面曝光…謝金河評比：金門坑道更壯觀，但DMZ有1大特色\",\n",
    "    \"第一名店董座王義郎辭世享壽86歲…從工友到銀行員、航空業總座再創業：我愛工作創新「一生都要學習」\",\n",
    "    \"中信金、富邦金…女星存金融股「無痛買iPhone 17」，60張兆豐金底氣超足：我是存錢的概念，不是賺價差\"\n",
    "]\n",
    "# 批次預測\n",
    "results = classifier(titles)\n",
    "for result in results:\n",
    "    print(f\"{result['label']} (信心: {result['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf5a47",
   "metadata": {},
   "source": [
    "英文新聞標題表現也不太優...但比中文好一丁點"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1355bbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL_1 (信心: 0.88)\n",
      "LABEL_1 (信心: 0.61)\n",
      "LABEL_1 (信心: 0.72)\n",
      "LABEL_0 (信心: 0.66)\n",
      "LABEL_0 (信心: 0.68)\n",
      "LABEL_0 (信心: 0.88)\n",
      "LABEL_1 (信心: 0.85)\n",
      "LABEL_1 (信心: 0.94)\n",
      "LABEL_1 (信心: 0.96)\n",
      "LABEL_1 (信心: 0.95)\n",
      "LABEL_0 (信心: 0.81)\n",
      "LABEL_1 (信心: 0.89)\n",
      "LABEL_0 (信心: 0.76)\n",
      "LABEL_0 (信心: 0.72)\n",
      "LABEL_0 (信心: 0.75)\n",
      "LABEL_0 (信心: 0.84)\n",
      "LABEL_1 (信心: 0.73)\n",
      "LABEL_0 (信心: 0.68)\n",
      "LABEL_1 (信心: 0.98)\n",
      "LABEL_0 (信心: 0.80)\n"
     ]
    }
   ],
   "source": [
    "# 新增一個 cell，將20個新聞標題丟入 fine-tuned BERT 模型，批次判斷正面/負面\n",
    "from transformers import pipeline\n",
    "# 載入 fine-tuned 模型\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"./fine_tuned_bert_max\")\n",
    "# 20則新聞標題\n",
    "titles = [\n",
    "\"Guangzhou's operations continue to report positive earnings, and the recovery is a reality.\",\n",
    "\n",
    "\"The world's most powerful investment advisor arrives! Trump calls for a substantial rate cut.\",\n",
    "\n",
    "\"Smart healthcare remains popular, and Avalue is targeting key growth opportunities.\",\n",
    "\n",
    "\"The Taiwan Drone Alliance launches! Which stocks are worth watching?\",\n",
    "\n",
    "\"Powertech sheds its image as a bull stock and becomes a favorite among institutional investors.\",\n",
    "\n",
    "\"Nvidia faces antitrust investigation in China: How should the market view it?\",\n",
    "\n",
    "\"Ten Taiwanese companies make the list! Time reveals its list of the world's best companies: Nvidia and Microsoft... prioritize AI, while Apple and TSMC are left out?\",\n",
    "\n",
    "\"iPhone 17 pre-orders are now open in Taiwan! A guide to the tariff plans of the three major telecom operators.\",\n",
    "\"Attending the 'College Student Rural Migration Competition'... Hsiao Mei-ching discusses five key youth support policies, encouraging students to bring Taiwan's rural spirit to the world.\",\n",
    "\n",
    "\"1.1264 Taishin Securities shares exchanged for 1 Yuanfu Securities share! Merger! Market share jumps to 4th place, futures merger becomes 5th... Lai Zhaoyin: Resource integration unlocks synergy.\",\n",
    "\"AI liquid cooling explodes! Shuanghong seizes the lead in liquid cooling.\",\n",
    "\"August EPS increases nearly 60% year-over-year! Lite-On Technology secures a position at the heart of power supply.\",\n",
    "\"Xingda Power Plant fire will lead to major power cuts? Taipower denies rumors, reviewing recent major blackouts.\",\n",
    "\"Third-generation wafers surge, SiC becomes the key solution.\",\n",
    "\"Want a clear mind all day? First thing in the morning: Never check your phone!\",\n",
    "\"Being a manager is tough! Quick changes are called fake, slow changes are called inactive. What's the right approach?\",\n",
    "\"Visiting the Demilitarized Zone and the 38th Parallel in South Korea! Xie Jinhe looks back at the Korean War: Is South Korea or Taiwan safer? One difference reveals the most dangerous country on Earth.\",\n",
    "\"Bending over in the tunnels of South Korea's Demilitarized Zone! Internal footage leaked... Xie Jinhe's assessment: The Kinmen tunnels are more spectacular, but the DMZ has one major feature.\",\n",
    "\"Wang Yilang, chairman of No. 1 Store, passed away at the age of 86... From worker to banker, then airline CEO, and then entrepreneur again: I love work and innovation, and I'm a lifelong learner.\",\n",
    "\"Chinatrust Financial Holdings, Fubon Financial Holdings... Actresses invest in financial stocks, making it a painless purchase of the iPhone 17. 60 Mega Financial Holdings shares offer confidence: I'm saving money, not profiting from price differences.\"\n",
    "]\n",
    "\n",
    "\n",
    "# 批次預測\n",
    "results = classifier(titles)\n",
    "for result in results:\n",
    "    print(f\"{result['label']} (信心: {result['score']:.2f})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
